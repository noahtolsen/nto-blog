[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hey there, I’m Noah, a passionate Data Scientist and Machine Learning Engineer living in Northfield, Vermont, with my fiancée Caroline and our two dogs, Leila and Honey. I love keeping up with all the progress and innovation in the ML world, though the pace of progress can be dizzying at times.\nThis blog is a place for me to write down my thoughts on what I’m reading and learning. I find that writing helps me retain information better. I hope you find my posts insightful and engaging.\nI’m always eager to connect with others in the industry. Feel free to reach out to me on LinkedIn, Twitter, or via email. Let’s connect and share our passion for data science and machine learning!"
  },
  {
    "objectID": "posts/job_alerts_pt1/index.html",
    "href": "posts/job_alerts_pt1/index.html",
    "title": "Building a Job Posting Tracker",
    "section": "",
    "text": "Hey there! I’ve been looking for jobs recently, and I’ve felt like I’ve spent a lot of time applying to positions that had already stopped accepting applications. This process has been exhausting and really taxing on my mental health and self-esteem. I was curious if I could build my own solution to identify relevant job postings at companies I would love to work at, soon after they get posted. Hopefully, this way I can get my applications in earlier in the cycle, increasing my chances of getting noticed by recruiters."
  },
  {
    "objectID": "posts/job_alerts_pt1/index.html#intro",
    "href": "posts/job_alerts_pt1/index.html#intro",
    "title": "Building a Job Posting Tracker",
    "section": "",
    "text": "Hey there! I’ve been looking for jobs recently, and I’ve felt like I’ve spent a lot of time applying to positions that had already stopped accepting applications. This process has been exhausting and really taxing on my mental health and self-esteem. I was curious if I could build my own solution to identify relevant job postings at companies I would love to work at, soon after they get posted. Hopefully, this way I can get my applications in earlier in the cycle, increasing my chances of getting noticed by recruiters."
  },
  {
    "objectID": "posts/job_alerts_pt1/index.html#development-plan",
    "href": "posts/job_alerts_pt1/index.html#development-plan",
    "title": "Building a Job Posting Tracker",
    "section": "Development Plan",
    "text": "Development Plan\nMy plan was to identify companies of interest (Microsoft, Google, and Amazon for this initial version), look into each company’s recruiting portal to figure out the information I need to extract job postings from the HTML, scrape each site, put the information for the most recently posted job into an email template, and run the whole cycle automatically once per day. To host the tracker, my plan was to containerize it with Docker, then host it using the Google Artifact Registry and run it using Google Cloud Run, with Cloud Scheduler determining the cadence."
  },
  {
    "objectID": "posts/job_alerts_pt1/index.html#development-walkthrough",
    "href": "posts/job_alerts_pt1/index.html#development-walkthrough",
    "title": "Building a Job Posting Tracker",
    "section": "Development Walkthrough",
    "text": "Development Walkthrough\nI won’t post the full code here but will provide enough to understand the process. The first step was to look into each company’s job portal and use Selenium and Beautiful Soup to load dynamic JavaScript content and extract the relevant information with BeautifulSoup. Here is an example of the process to do that for Amazon.\n\nScraping Job Postings\nThe first step is to create a Selenium WebDriver with all the configuration options needed to run a headless Chrome browser.\n\nfrom selenium.webdriver.common.by import By\nfrom bs4 import BeautifulSoup\nimport logging\nimport time\nfrom selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium import webdriver\nfrom datetime import datetime\nimport pprint\n\ndef create_webdriver():\n    logging.info(\"Creating WebDriver instance\")\n    chrome_options = Options()\n    chrome_options.add_argument(\"--headless\")\n    chrome_options.add_argument(\"--no-sandbox\")\n    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n    chrome_options.add_argument(\"--disable-gpu\")\n    chrome_options.add_argument(\"--window-size=1920x1080\")\n\n    try:\n        driver = webdriver.Chrome(options=chrome_options)\n        logging.info(\"WebDriver instance created successfully\")\n        return driver\n    except Exception as e:\n        logging.error(f\"Error connecting to WebDriver: {e}\")\n        raise\n\nNext, we have the code to actually visit the Amazon job portal with URL parameters to prefilter for the types of jobs I am interested in.\n\ndef get_job_postings_amazon(driver, url='https://www.amazon.jobs/en/search?offset=0&result_limit=10&sort=recent&country%5B%5D=USA&state%5B%5D=Washington&state%5B%5D=California&state%5B%5D=Texas&state%5B%5D=Massachusetts&state%5B%5D=Colorado&distanceType=Mi&radius=24km&latitude=&longitude=&loc_group_id=&loc_query=&base_query=Data%20Scientist&city=&country=&region=&county=&query_options=&'):\n    logging.info(f\"Fetching job postings from Amazon: {url}\")\n    driver.get(url)\n    time.sleep(5)  # Wait for the page to fully load\n    page_source = driver.page_source\n    soup = BeautifulSoup(page_source, 'html.parser')\n    \n    jobs = []\n    today = datetime.now().date()\n    job_tiles = soup.find_all('div', class_='job-tile')\n    \n    logging.info(f\"Found {len(job_tiles)} job tiles on the page\")\n\n    for job in job_tiles:\n        title_element = job.find('h3', class_='job-title')\n        location_element = job.find('div', class_='location-and-id')\n        link_element = job.find('a', class_='read-more')\n        posted_date_element = job.find('h2', class_=\"posting-date\")\n        description_element = job.find('div', class_=\"description\")\n        if not title_element or not location_element or not link_element or not posted_date_element:\n            logging.warning(f\"Skipping job due to missing elements: {job}\")\n            continue\n\n        title = title_element.text.strip()\n        location = location_element.text.strip().replace('Locations', '').split('|')[0]\n        link = 'https://www.amazon.jobs' + link_element['href']\n        posted_date_str = posted_date_element.text.replace('Posted ', '').strip()\n        description = description_element.text.strip()\n        \n        # Parse the posted date\n        try:\n            posted_date = datetime.strptime(posted_date_str, '%B %d, %Y').date()\n        except ValueError:\n            logging.warning(f\"Skipping job with unrecognized date format: {posted_date_str}\")\n            continue\n\n        # Log dates for debugging\n        logging.info(f\"Today: {today}, Posted Date: {posted_date}, Difference: {(today - posted_date).days} days\")\n\n        # Only consider jobs posted today or yesterday\n        if (today - posted_date).days &gt; 4:\n            logging.info(f\"Skipping job posted on {posted_date_str} (more than 1 day old)\")\n            continue\n        \n        jobs.append({\n            'title': title,\n            'location': location,\n            'link': link,\n            'posted-date': posted_date_str,\n            'overview-text': description\n        })\n    \n    logging.info(f\"Found {len(jobs)} relevant job postings from Amazon\")\n    return {'Amazon': jobs}\n\nFinally, we run the code and print out the jobs that were pulled from the portal.\n\ndriver = create_webdriver()\namazon_jobs = get_job_postings_amazon(driver)\npprint.pp(amazon_jobs['Amazon'][0]) #Only printing the first job posting for legibility.\n\n{'title': 'Sr. Data Scientist , SCOT - AIM - NA',\n 'location': 'Bellevue, WA, USA',\n 'link': 'https://www.amazon.jobs/en/jobs/2899067/sr-data-scientist-scot-aim-na',\n 'posted-date': 'February 12, 2025',\n 'overview-text': 'Basic qualifications:7+ years of data scientist or similar '\n                  'role involving data extraction, analysis, statistical '\n                  \"modeling and communication experienceMaster's degree in a \"\n                  'quantitative field such as statistics, mathematics, data '\n                  'science, business analytics, economics, finance, '\n                  'engineering, or computer science5+ years of data querying '\n                  'languages (e.g. SQL), scripting languages (e.g. Python) or '\n                  'statistical/mathematical software (e.g. R, SAS, Matlab, '\n                  'etc.) experience...Read more'}\n\n\n\n\nSending the email\nNow that we have some job postings to work with, we can put that into an email template and send the results.\n\nimport smtplib\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nimport logging\nfrom datetime import datetime\nimport pytz\nfrom dotenv import load_dotenv\nimport os\nload_dotenv('_environment.local')\n\ndef send_email(job_postings, sender_email, receiver_email, email_password):\n    tz = pytz.timezone('America/New_York')\n    now = datetime.now(tz)\n    current_time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    # Create the email content\n    message = MIMEMultipart('alternative')\n    message['Subject'] = 'New Job Postings Alert - ' + current_time\n    message['From'] = sender_email\n    message['To'] = receiver_email\n\n    html = \"\"\"\n    &lt;html&gt;\n    &lt;head&gt;\n        &lt;style&gt;\n            body {\n                font-family: Arial, sans-serif;\n                background-color: #f4f4f9;\n                margin: 0;\n                padding: 0;\n            }\n            .container {\n                width: 80%;\n                margin: 0 auto;\n                background-color: #fff;\n                padding: 20px;\n                box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);\n                border-radius: 8px;\n            }\n            h2, h3 {\n                color: #333;\n            }\n            ul {\n                list-style-type: none;\n                padding: 0;\n            }\n            li {\n                background-color: #fafafa;\n                margin: 10px 0;\n                padding: 10px;\n                border-radius: 4px;\n                border: 1px solid #ddd;\n            }\n            a {\n                text-decoration: none;\n                color: #0366d6;\n                font-weight: bold;\n            }\n            a:hover {\n                text-decoration: underline;\n            }\n            .job-title {\n                font-size: 16px;\n                margin: 0;\n            }\n            .job-details {\n                color: #555;\n                font-size: 14px;\n            }\n        &lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;div class=\"container\"&gt;\n            &lt;h2&gt;New Job Postings&lt;/h2&gt;\n    \"\"\"\n\n    # Debugging: Track the HTML generation\n    logging.info(\"Generating email HTML content\")\n\n    for company, jobs in job_postings.items():\n        html += f\"&lt;h3&gt;{company}&lt;/h3&gt;&lt;ul&gt;\"\n        for job in jobs:\n            overview_text = job['overview-text']\n            if len(overview_text) &gt; 200:  # Truncate to 200 characters\n                overview_text = overview_text[:200] + '...'\n\n            job_html = f\"\"\"\n            &lt;li&gt;\n                &lt;p class=\"job-title\"&gt;&lt;a href='{job['link']}'&gt;{job['title']}&lt;/a&gt;&lt;/p&gt;\n                &lt;p class=\"job-details\"&gt;{job['location']}&lt;br&gt;Posted: {job['posted-date']}&lt;br&gt;{overview_text}&lt;/p&gt;\n            &lt;/li&gt;\n            \"\"\"\n            html += job_html\n            logging.info(f\"Added job HTML: {job_html.strip()}\")  # Debugging: Log each job HTML\n\n        html += \"&lt;/ul&gt;\"\n\n    html += \"\"\"\n        &lt;/div&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n    \"\"\"\n\n    logging.info(f\"Final email HTML content length: {len(html)}\")  # Debugging: Log final HTML length\n\n    message.attach(MIMEText(html, 'html'))\n\n    # Send the email\n    try:\n        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:\n            server.login(sender_email, email_password)\n            server.sendmail(sender_email, receiver_email, message.as_string())\n        logging.info(\"Email sent successfully\")\n    except Exception as e:\n        logging.error(f\"Failed to send email: {e}\")\n\njob_postings = {}\njob_postings.update(amazon_jobs)\nsend_email(job_postings, os.getenv('SENDER_EMAIL'), os.getenv('RECEIVER_EMAIL'), os.getenv('EMAIL_PASSWORD'))\n\nAnd here is a screenshot of the email that was sent.\n\n\n\nDeployment\nNow that we have working code to scrape a job portal and a function to format it and send it as an email, we need a plan to deploy the project. To deploy the project, I containerized the app using Docker and then used a combination of GCP Cloud Run to run the container and Cloud Scheduler to run it once a day on a cron schedule."
  },
  {
    "objectID": "posts/job_alerts_pt1/index.html#future-directions",
    "href": "posts/job_alerts_pt1/index.html#future-directions",
    "title": "Building a Job Posting Tracker",
    "section": "Future Directions",
    "text": "Future Directions\nI definitely don’t see this as a finished product. In the future, I plan to continue adding new companies; there are just so many cool companies doing exciting, groundbreaking work in machine learning and data science. I also have plans to integrate the Notion API to send the scraped posts directly to the Notion database I use to organize my job applications. I would also like to experiment with integrating the OpenAI API to have GPT-4 score each job posting based on how qualified I am for the position by comparing each job to my resume. So stay tuned for a post on those updates!"
  },
  {
    "objectID": "posts/quantization/index.html",
    "href": "posts/quantization/index.html",
    "title": "Quantization of LLMs",
    "section": "",
    "text": "I recently had a job interview where I was asked to go into the technical details about the quantization of LLMs. It had been a while since I had looked at quantization so I blanked a little bit and the details that I did provide were wrong. So I thought this would be a great moment for a blog post on quantization to drive it back into my own brain."
  },
  {
    "objectID": "posts/quantization/index.html#intro",
    "href": "posts/quantization/index.html#intro",
    "title": "Quantization of LLMs",
    "section": "",
    "text": "I recently had a job interview where I was asked to go into the technical details about the quantization of LLMs. It had been a while since I had looked at quantization so I blanked a little bit and the details that I did provide were wrong. So I thought this would be a great moment for a blog post on quantization to drive it back into my own brain."
  },
  {
    "objectID": "posts/quantization/index.html#high-level-background",
    "href": "posts/quantization/index.html#high-level-background",
    "title": "Quantization of LLMs",
    "section": "High Level Background",
    "text": "High Level Background\nThe main goal of quantization is to make large language models (LLMs) more efficient by reducing their memory footprint and computational requirement. As LLMs have continued to improve and model developers pursue the scaling theory that more parameters leads to more performance, many of the state of the art models cannot be run for inference efficiently with the standard computer hardware that the vast majority of people have access to.\nFor example, Meta’s largest and most advanced model Llama 3.1 405B would require at least 8 NVIDIA A100 GPUs, far beyond what most people dabbling with LLMs have access to.\nThe idea behind quantization is to reduce the memory requirements of running inference on a model by lowering the precision of model weights from FP32 to FP8. FP32 allows for high numerical precision, but this comes at the cost of increased memory usage. This tradeoff becomes particularly challenging at scale—for example, storing 405 billion FP32 weights in a massive LLM requires enormous memory and computational resources."
  },
  {
    "objectID": "posts/quantization/index.html#what-is-floating-point-precision",
    "href": "posts/quantization/index.html#what-is-floating-point-precision",
    "title": "Quantization of LLMs",
    "section": "What is Floating Point Precision?",
    "text": "What is Floating Point Precision?\nNow that we have a basic understanding of what we are trying to do with quantization, it is important to understand the concepts behind floating point precision and what characteristics make different Floating Point Precision formats different for the sake of LLM inference."
  },
  {
    "objectID": "posts/quantization/index.html#extra-section",
    "href": "posts/quantization/index.html#extra-section",
    "title": "Quantization of LLMs",
    "section": "Extra Section",
    "text": "Extra Section"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome! Nothing really to talk about yet, so enjoy some photos of my dogs Leila and Honey."
  },
  {
    "objectID": "posts/homl_ch2/index.html",
    "href": "posts/homl_ch2/index.html",
    "title": "Hands on Machine Learning Chapter 2",
    "section": "",
    "text": "Hey there! I’ve been rereading Aurélien Géron’s excellent book Hands-On Machine Learning with Scikit-Learn, Keras & Tensorflow.\n\nIt’s a fantastic book that a highly recommend taking a look at. I’m going to go back through the book talking a little about each chapter and going through the exercises at the end of each chapter.\nChapter 2 focused on an example end to end machine learning project/workflow. The dataset used was the California Housing dataset, a common regression dataset in ML. The task is to predict the value of an unseen house based on district level housing data in California. Here is a sample of the dataset.\n\n\n\n\n    \n      \n      longitude\n      latitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      median_house_value\n      ocean_proximity\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.1.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "posts/homl_ch2/index.html#intro",
    "href": "posts/homl_ch2/index.html#intro",
    "title": "Hands on Machine Learning Chapter 2",
    "section": "",
    "text": "Hey there! I’ve been rereading Aurélien Géron’s excellent book Hands-On Machine Learning with Scikit-Learn, Keras & Tensorflow.\n\nIt’s a fantastic book that a highly recommend taking a look at. I’m going to go back through the book talking a little about each chapter and going through the exercises at the end of each chapter.\nChapter 2 focused on an example end to end machine learning project/workflow. The dataset used was the California Housing dataset, a common regression dataset in ML. The task is to predict the value of an unseen house based on district level housing data in California. Here is a sample of the dataset.\n\n\n\n\n    \n      \n      longitude\n      latitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      median_house_value\n      ocean_proximity\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.1.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "posts/homl_ch2/index.html#chapter-summary",
    "href": "posts/homl_ch2/index.html#chapter-summary",
    "title": "Hands on Machine Learning Chapter 2",
    "section": "Chapter Summary",
    "text": "Chapter Summary\nAfter putting together a preprocessing pipeline that’s summarized in the model graph below, we can start with the simplest possible model, a linear regression.\n\nfrom sklearn.linear_model import LinearRegression\n\npreprocessing = ColumnTransformer([\n    (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n    (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n    (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n    (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\", \"households\", \"median_income\"]),\n    (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n    (\"cat\", cat_pipeline, make_column_selector(dtype_include=object))],\n    remainder=default_num_pipeline)\n\nhousing_prepared = preprocessing.fit_transform(housing)\n\nlin_reg = make_pipeline(preprocessing, LinearRegression())\nlin_reg.fit(housing, housing_labels)\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                              SimpleImputer(strategy='median')),\n                                                             ('standardscaler',\n                                                              StandardScaler())]),\n                                   transformers=[('bedrooms',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('functiontransformer',\n                                                                   FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x15a...\n                                                   'median_income']),\n                                                 ('geo',\n                                                  ClusterSimilarity(gamma=0.1,\n                                                                    random_state=42),\n                                                  ['latitude', 'longitude']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x15a74fad0&gt;)])),\n                ('linearregression', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                              SimpleImputer(strategy='median')),\n                                                             ('standardscaler',\n                                                              StandardScaler())]),\n                                   transformers=[('bedrooms',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('functiontransformer',\n                                                                   FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x15a...\n                                                   'median_income']),\n                                                 ('geo',\n                                                  ClusterSimilarity(gamma=0.1,\n                                                                    random_state=42),\n                                                  ['latitude', 'longitude']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x15a74fad0&gt;)])),\n                ('linearregression', LinearRegression())])  columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformerColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                             SimpleImputer(strategy='median')),\n                                            ('standardscaler',\n                                             StandardScaler())]),\n                  transformers=[('bedrooms',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('functiontransformer',\n                                                  FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x15a7e7740&gt;,\n                                                                      func=&lt;function column_ratio at 0...\n                                 ['total_bedrooms', 'total_rooms', 'population',\n                                  'households', 'median_income']),\n                                ('geo',\n                                 ClusterSimilarity(gamma=0.1, random_state=42),\n                                 ['latitude', 'longitude']),\n                                ('cat',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x15a74fad0&gt;)]) bedrooms['total_bedrooms', 'total_rooms']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  FunctionTransformer?Documentation for FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x15a7e7740&gt;,\n                    func=&lt;function column_ratio at 0x15a34a340&gt;)  StandardScaler?Documentation for StandardScalerStandardScaler() rooms_per_house['total_rooms', 'households']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  FunctionTransformer?Documentation for FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x15a7e7740&gt;,\n                    func=&lt;function column_ratio at 0x15a34a340&gt;)  StandardScaler?Documentation for StandardScalerStandardScaler() people_per_house['population', 'households']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  FunctionTransformer?Documentation for FunctionTransformerFunctionTransformer(feature_names_out=&lt;function ratio_name at 0x15a7e7740&gt;,\n                    func=&lt;function column_ratio at 0x15a34a340&gt;)  StandardScaler?Documentation for StandardScalerStandardScaler() log['total_bedrooms', 'total_rooms', 'population', 'households', 'median_income']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  FunctionTransformer?Documentation for FunctionTransformerFunctionTransformer(feature_names_out='one-to-one', func=&lt;ufunc 'log'&gt;)  StandardScaler?Documentation for StandardScalerStandardScaler() geo['latitude', 'longitude'] ClusterSimilarityClusterSimilarity(gamma=0.1, random_state=42) cat&lt;sklearn.compose._column_transformer.make_column_selector object at 0x15a74fad0&gt;  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') remainder['housing_median_age']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler()  LinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\nNow how does this simple model perform?\n\nfrom sklearn.metrics import root_mean_squared_error\n\nhousing_predictions = lin_reg.predict(housing)\nlin_rmse = root_mean_squared_error(housing_labels, housing_predictions)\n\nprint(f\"The average prediction error for the linear regression model\\nas measured by the root mean squared error is ${lin_rmse:,.2f}\")\n\nThe average prediction error for the linear regression model\nas measured by the root mean squared error is $67,825.01\n\n\nSo clearly this model does not have particularly strong results. Next we can try a decision tree model.\n\nfrom sklearn.tree import DecisionTreeRegressor\n\ntree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))\ntree_reg.fit(housing, housing_labels)\nhousing_predictions = tree_reg.predict(housing)\ntree_rmse = root_mean_squared_error(housing_labels, housing_predictions)\nprint(f\"The average prediction error for the decision tree model\\nas measured by the root mean squared error is ${tree_rmse:,.2f}\")\n\nThe average prediction error for the decision tree model\nas measured by the root mean squared error is $0.00\n\n\nWow! A perfect model, that’s great! Not so fast, the first rule of machine learning is to always treat results that seem to good to be true. While our previous model was underfitting the data, not providing enough insight to offer good predictions. This model is overfitting, learning the training data too well. While it can perfectly predict a value when we give it results it has already seen (which is what we are doing here), it will generalize very poorly to new data.\nLet’s try using scikit-learn’s k fold cross validation function. Here, we split the data in to k non-overlapping subsets (folds) and train the decision tree model k times, each time holding out a different fold for model evaluation and using the other k-1 to train the model. In the end you end up with an array with ten evaluation scores.\n\nfrom sklearn.model_selection import cross_val_score\n\ntree_rmses = -cross_val_score(tree_reg, housing, housing_labels, \n                                scoring=\"neg_root_mean_squared_error\", cv=10)\npd.Series(tree_rmses).describe()\n\ncount       10.000000\nmean     66719.534520\nstd       1108.501177\nmin      64929.167182\n25%      66206.831015\n50%      66524.005161\n75%      67021.373805\nmax      68566.339092\ndtype: float64\n\n\nNow the model looks less than perfect. In fact, the average root mean squared error of $66,719.53 is almost as bad as the linear regression model. So clearly we need to look further. A good place to look next is a Random Forest model. Random Forest is an extension of Decision Trees where we train many different decision trees on subsets of the training data and average out their results. They are very simply to train in scikit-learn.\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nforest_reg = make_pipeline(preprocessing,\n                           RandomForestRegressor(random_state=42))\n\nforest_rmses = -cross_val_score(forest_reg, housing, housing_labels, \n                                scoring=\"neg_root_mean_squared_error\", cv=10)\npd.Series(forest_rmses).describe()\n\ncount       10.000000\nmean     47018.993979\nstd       1042.481972\nmin      45707.317044\n25%      46514.403841\n50%      47017.225303\n75%      47243.182273\nmax      49343.424080\ndtype: float64\n\n\nThat’s already an improvement of at almost $20,000! But can we do better?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Noah’s Blog",
    "section": "",
    "text": "Quantization of LLMs\n\n\n\n\n\n\nGenerative AI\n\n\nMachine Learning\n\n\nDRAFT\n\n\n\n\n\n\n\n\n\nFeb 15, 2025\n\n\nNoah Olsen\n\n\n\n\n\n\n\n\n\n\n\n\nHands on Machine Learning Chapter 2\n\n\n\n\n\n\nHOML\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\nNoah Olsen\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Job Posting Tracker\n\n\n\n\n\n\njob tracker\n\n\nweb scraping\n\n\njob applications\n\n\ngcp\n\n\ndocker\n\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\nNoah Olsen\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMay 19, 2024\n\n\nNoah Olsen\n\n\n\n\n\n\nNo matching items"
  }
]