[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hey there, I’m Noah, a passionate Data Scientist and Machine Learning Engineer living in Northfield, Vermont, with my fiancée Caroline and our two dogs, Leila and Honey. I love keeping up with all the progress and innovation in the ML world, though the pace of progress can be dizzying at times.\nThis blog is a place for me to write down my thoughts on what I’m reading and learning. I find that writing helps me retain information better. I hope you find my posts insightful and engaging.\nI’m always eager to connect with others in the industry. Feel free to reach out to me on LinkedIn, Twitter, or via email. Let’s connect and share our passion for data science and machine learning!"
  },
  {
    "objectID": "posts/job_alerts_pt1/index.html",
    "href": "posts/job_alerts_pt1/index.html",
    "title": "Building a Job Posting Tracker",
    "section": "",
    "text": "Hey there! I’ve been looking for jobs recently, and I’ve felt like I’ve spent a lot of time applying to positions that had already stopped accepting applications. This process has been exhausting and really taxing on my mental health and self-esteem. I was curious if I could build my own solution to identify relevant job postings at companies I would love to work at, soon after they get posted. Hopefully, this way I can get my applications in earlier in the cycle, increasing my chances of getting noticed by recruiters."
  },
  {
    "objectID": "posts/job_alerts_pt1/index.html#intro",
    "href": "posts/job_alerts_pt1/index.html#intro",
    "title": "Building a Job Posting Tracker",
    "section": "",
    "text": "Hey there! I’ve been looking for jobs recently, and I’ve felt like I’ve spent a lot of time applying to positions that had already stopped accepting applications. This process has been exhausting and really taxing on my mental health and self-esteem. I was curious if I could build my own solution to identify relevant job postings at companies I would love to work at, soon after they get posted. Hopefully, this way I can get my applications in earlier in the cycle, increasing my chances of getting noticed by recruiters."
  },
  {
    "objectID": "posts/job_alerts_pt1/index.html#development-plan",
    "href": "posts/job_alerts_pt1/index.html#development-plan",
    "title": "Building a Job Posting Tracker",
    "section": "Development Plan",
    "text": "Development Plan\nMy plan was to identify companies of interest (Microsoft, Google, and Amazon for this initial version), look into each company’s recruiting portal to figure out the information I need to extract job postings from the HTML, scrape each site, put the information for the most recently posted job into an email template, and run the whole cycle automatically once per day. To host the tracker, my plan was to containerize it with Docker, then host it using the Google Artifact Registry and run it using Google Cloud Run, with Cloud Scheduler determining the cadence."
  },
  {
    "objectID": "posts/job_alerts_pt1/index.html#development-walkthrough",
    "href": "posts/job_alerts_pt1/index.html#development-walkthrough",
    "title": "Building a Job Posting Tracker",
    "section": "Development Walkthrough",
    "text": "Development Walkthrough\nI won’t post the full code here but will provide enough to understand the process. The first step was to look into each company’s job portal and use Selenium and Beautiful Soup to load dynamic JavaScript content and extract the relevant information with BeautifulSoup. Here is an example of the process to do that for Amazon.\n\nScraping Job Postings\nThe first step is to create a Selenium WebDriver with all the configuration options needed to run a headless Chrome browser.\n\nfrom selenium.webdriver.common.by import By\nfrom bs4 import BeautifulSoup\nimport logging\nimport time\nfrom selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium import webdriver\nfrom datetime import datetime\nimport pprint\n\ndef create_webdriver():\n    logging.info(\"Creating WebDriver instance\")\n    chrome_options = Options()\n    chrome_options.add_argument(\"--headless\")\n    chrome_options.add_argument(\"--no-sandbox\")\n    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n    chrome_options.add_argument(\"--disable-gpu\")\n    chrome_options.add_argument(\"--window-size=1920x1080\")\n\n    try:\n        driver = webdriver.Chrome(options=chrome_options)\n        logging.info(\"WebDriver instance created successfully\")\n        return driver\n    except Exception as e:\n        logging.error(f\"Error connecting to WebDriver: {e}\")\n        raise\n\nNext, we have the code to actually visit the Amazon job portal with URL parameters to prefilter for the types of jobs I am interested in.\n\ndef get_job_postings_amazon(driver, url='https://www.amazon.jobs/en/search?offset=0&result_limit=10&sort=recent&country%5B%5D=USA&state%5B%5D=Washington&state%5B%5D=California&state%5B%5D=Texas&state%5B%5D=Massachusetts&state%5B%5D=Colorado&distanceType=Mi&radius=24km&latitude=&longitude=&loc_group_id=&loc_query=&base_query=Data%20Scientist&city=&country=&region=&county=&query_options=&'):\n    logging.info(f\"Fetching job postings from Amazon: {url}\")\n    driver.get(url)\n    time.sleep(5)  # Wait for the page to fully load\n    page_source = driver.page_source\n    soup = BeautifulSoup(page_source, 'html.parser')\n    \n    jobs = []\n    today = datetime.now().date()\n    job_tiles = soup.find_all('div', class_='job-tile')\n    \n    logging.info(f\"Found {len(job_tiles)} job tiles on the page\")\n\n    for job in job_tiles:\n        title_element = job.find('h3', class_='job-title')\n        location_element = job.find('div', class_='location-and-id')\n        link_element = job.find('a', class_='read-more')\n        posted_date_element = job.find('h2', class_=\"posting-date\")\n        description_element = job.find('div', class_=\"description\")\n        if not title_element or not location_element or not link_element or not posted_date_element:\n            logging.warning(f\"Skipping job due to missing elements: {job}\")\n            continue\n\n        title = title_element.text.strip()\n        location = location_element.text.strip().replace('Locations', '').split('|')[0]\n        link = 'https://www.amazon.jobs' + link_element['href']\n        posted_date_str = posted_date_element.text.replace('Posted ', '').strip()\n        description = description_element.text.strip()\n        \n        # Parse the posted date\n        try:\n            posted_date = datetime.strptime(posted_date_str, '%B %d, %Y').date()\n        except ValueError:\n            logging.warning(f\"Skipping job with unrecognized date format: {posted_date_str}\")\n            continue\n\n        # Log dates for debugging\n        logging.info(f\"Today: {today}, Posted Date: {posted_date}, Difference: {(today - posted_date).days} days\")\n\n        # Only consider jobs posted today or yesterday\n        if (today - posted_date).days &gt; 4:\n            logging.info(f\"Skipping job posted on {posted_date_str} (more than 1 day old)\")\n            continue\n        \n        jobs.append({\n            'title': title,\n            'location': location,\n            'link': link,\n            'posted-date': posted_date_str,\n            'overview-text': description\n        })\n    \n    logging.info(f\"Found {len(jobs)} relevant job postings from Amazon\")\n    return {'Amazon': jobs}\n\nFinally, we run the code and print out the jobs that were pulled from the portal.\n\ndriver = create_webdriver()\namazon_jobs = get_job_postings_amazon(driver)\npprint.pp(amazon_jobs['Amazon'][0]) #Only printing the first job posting for legibility.\n\n{'title': 'Data Scientist II',\n 'location': 'Bellevue, WA, USASeattle, WA, USA+1 other locations  ',\n 'link': 'https://www.amazon.jobs/en/jobs/2651002/data-scientist-ii',\n 'posted-date': 'May 22, 2024',\n 'overview-text': 'Basic qualifications:3+ years of data scientist '\n                  'experience3+ years of data querying languages (e.g. SQL), '\n                  'scripting languages (e.g. Python) or '\n                  'statistical/mathematical software (e.g. R, SAS, Matlab, '\n                  'etc.) experience3+ years of machine learning/statistical '\n                  'modeling data analysis tools and techniques, and parameters '\n                  'that affect their performance experienceExperience applying '\n                  \"theoretical models in an applied environmentMaster's \"\n                  'degree...Read more'}\n\n\n\n\nSending the email\nNow that we have some job postings to work with, we can put that into an email template and send the results.\n\nimport smtplib\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nimport logging\nfrom datetime import datetime\nimport pytz\nfrom dotenv import load_dotenv\nimport os\nload_dotenv('_environment.local')\n\ndef send_email(job_postings, sender_email, receiver_email, email_password):\n    tz = pytz.timezone('America/New_York')\n    now = datetime.now(tz)\n    current_time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    # Create the email content\n    message = MIMEMultipart('alternative')\n    message['Subject'] = 'New Job Postings Alert - ' + current_time\n    message['From'] = sender_email\n    message['To'] = receiver_email\n\n    html = \"\"\"\n    &lt;html&gt;\n    &lt;head&gt;\n        &lt;style&gt;\n            body {\n                font-family: Arial, sans-serif;\n                background-color: #f4f4f9;\n                margin: 0;\n                padding: 0;\n            }\n            .container {\n                width: 80%;\n                margin: 0 auto;\n                background-color: #fff;\n                padding: 20px;\n                box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);\n                border-radius: 8px;\n            }\n            h2, h3 {\n                color: #333;\n            }\n            ul {\n                list-style-type: none;\n                padding: 0;\n            }\n            li {\n                background-color: #fafafa;\n                margin: 10px 0;\n                padding: 10px;\n                border-radius: 4px;\n                border: 1px solid #ddd;\n            }\n            a {\n                text-decoration: none;\n                color: #0366d6;\n                font-weight: bold;\n            }\n            a:hover {\n                text-decoration: underline;\n            }\n            .job-title {\n                font-size: 16px;\n                margin: 0;\n            }\n            .job-details {\n                color: #555;\n                font-size: 14px;\n            }\n        &lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;div class=\"container\"&gt;\n            &lt;h2&gt;New Job Postings&lt;/h2&gt;\n    \"\"\"\n\n    # Debugging: Track the HTML generation\n    logging.info(\"Generating email HTML content\")\n\n    for company, jobs in job_postings.items():\n        html += f\"&lt;h3&gt;{company}&lt;/h3&gt;&lt;ul&gt;\"\n        for job in jobs:\n            overview_text = job['overview-text']\n            if len(overview_text) &gt; 200:  # Truncate to 200 characters\n                overview_text = overview_text[:200] + '...'\n\n            job_html = f\"\"\"\n            &lt;li&gt;\n                &lt;p class=\"job-title\"&gt;&lt;a href='{job['link']}'&gt;{job['title']}&lt;/a&gt;&lt;/p&gt;\n                &lt;p class=\"job-details\"&gt;{job['location']}&lt;br&gt;Posted: {job['posted-date']}&lt;br&gt;{overview_text}&lt;/p&gt;\n            &lt;/li&gt;\n            \"\"\"\n            html += job_html\n            logging.info(f\"Added job HTML: {job_html.strip()}\")  # Debugging: Log each job HTML\n\n        html += \"&lt;/ul&gt;\"\n\n    html += \"\"\"\n        &lt;/div&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n    \"\"\"\n\n    logging.info(f\"Final email HTML content length: {len(html)}\")  # Debugging: Log final HTML length\n\n    message.attach(MIMEText(html, 'html'))\n\n    # Send the email\n    try:\n        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:\n            server.login(sender_email, email_password)\n            server.sendmail(sender_email, receiver_email, message.as_string())\n        logging.info(\"Email sent successfully\")\n    except Exception as e:\n        logging.error(f\"Failed to send email: {e}\")\n\njob_postings = {}\njob_postings.update(amazon_jobs)\nsend_email(job_postings, os.getenv('SENDER_EMAIL'), os.getenv('RECEIVER_EMAIL'), os.getenv('EMAIL_PASSWORD'))\n\nAnd here is a screenshot of the email that was sent.\n\n\n\nDeployment\nNow that we have working code to scrape a job portal and a function to format it and send it as an email, we need a plan to deploy the project. To deploy the project, I containerized the app using Docker and then used a combination of GCP Cloud Run to run the container and Cloud Scheduler to run it once a day on a cron schedule."
  },
  {
    "objectID": "posts/job_alerts_pt1/index.html#future-directions",
    "href": "posts/job_alerts_pt1/index.html#future-directions",
    "title": "Building a Job Posting Tracker",
    "section": "Future Directions",
    "text": "Future Directions\nI definitely don’t see this as a finished product. In the future, I plan to continue adding new companies; there are just so many cool companies doing exciting, groundbreaking work in machine learning and data science. I also have plans to integrate the Notion API to send the scraped posts directly to the Notion database I use to organize my job applications. I would also like to experiment with integrating the OpenAI API to have GPT-4 score each job posting based on how qualified I am for the position by comparing each job to my resume. So stay tuned for a post on those updates!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome! Nothing really to talk about yet, so enjoy some photos of my dogs Leila and Honey."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Noah’s Blog",
    "section": "",
    "text": "Building a Job Posting Tracker\n\n\n\n\n\n\njob tracker\n\n\nweb scraping\n\n\njob applications\n\n\ngcp\n\n\ndocker\n\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\nNoah Olsen\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMay 19, 2024\n\n\nNoah Olsen\n\n\n\n\n\n\nNo matching items"
  }
]