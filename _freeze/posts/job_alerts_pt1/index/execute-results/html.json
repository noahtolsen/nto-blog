{
  "hash": "88167733b69f5fc304e3dd7bdf93c065",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Building a Job Posting Tracker\nauthor: Noah Olsen\ndate: '2024-05-24'\ncategories:\n  - job tracker\n  - web scraping\n  - job applications\n  - gcp\n  - docker\n---\n\n## Intro\nHey there! I've been looking for jobs recently, and I've felt like I've spent a lot of time applying to positions that had already stopped accepting applications. This process has been exhausting and really taxing on my mental health and self-esteem. I was curious if I could build my own solution to identify relevant job postings at companies I would love to work at, soon after they get posted. Hopefully, this way I can get my applications in earlier in the cycle, increasing my chances of getting noticed by recruiters.\n\n## Development Plan\nMy plan was to identify companies of interest (Microsoft, Google, and Amazon for this initial version), look into each company's recruiting portal to figure out the information I need to extract job postings from the HTML, scrape each site, put the information for the most recently posted job into an email template, and run the whole cycle automatically once per day. To host the tracker, my plan was to containerize it with Docker, then host it using the Google Artifact Registry and run it using Google Cloud Run, with Cloud Scheduler determining the cadence.\n\n## Development Walkthrough\nI won't post the full code here but will provide enough to understand the process. The first step was to look into each company's job portal and use Selenium and Beautiful Soup to load dynamic JavaScript content and extract the relevant information with BeautifulSoup. Here is an example of the process to do that for Amazon.\n\n### Scraping Job Postings\n\nThe first step is to create a Selenium WebDriver with all the configuration options needed to run a headless Chrome browser.\n\n::: {#b511ffac .cell execution_count=1}\n``` {.python .cell-code}\nfrom selenium.webdriver.common.by import By\nfrom bs4 import BeautifulSoup\nimport logging\nimport time\nfrom selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium import webdriver\nfrom datetime import datetime\nimport pprint\n\ndef create_webdriver():\n    logging.info(\"Creating WebDriver instance\")\n    chrome_options = Options()\n    chrome_options.add_argument(\"--headless\")\n    chrome_options.add_argument(\"--no-sandbox\")\n    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n    chrome_options.add_argument(\"--disable-gpu\")\n    chrome_options.add_argument(\"--window-size=1920x1080\")\n\n    try:\n        driver = webdriver.Chrome(options=chrome_options)\n        logging.info(\"WebDriver instance created successfully\")\n        return driver\n    except Exception as e:\n        logging.error(f\"Error connecting to WebDriver: {e}\")\n        raise\n```\n:::\n\n\nNext, we have the code to actually visit the [Amazon job portal](https://www.amazon.jobs/en/search?offset=0&result_limit=10&sort=recent&country%5B%5D=USA&state%5B%5D=Washington&state%5B%5D=California&state%5B%5D=Texas&state%5B%5D=Massachusetts&state%5B%5D=Colorado&distanceType=Mi&radius=24km&latitude=&longitude=&loc_group_id=&loc_query=&base_query=Data%20Scientist&city=&country=&region=&county=&query_options=&) with URL parameters to prefilter for the types of jobs I am interested in.\n\n::: {#190da189 .cell execution_count=2}\n``` {.python .cell-code}\ndef get_job_postings_amazon(driver, url='https://www.amazon.jobs/en/search?offset=0&result_limit=10&sort=recent&country%5B%5D=USA&state%5B%5D=Washington&state%5B%5D=California&state%5B%5D=Texas&state%5B%5D=Massachusetts&state%5B%5D=Colorado&distanceType=Mi&radius=24km&latitude=&longitude=&loc_group_id=&loc_query=&base_query=Data%20Scientist&city=&country=&region=&county=&query_options=&'):\n    logging.info(f\"Fetching job postings from Amazon: {url}\")\n    driver.get(url)\n    time.sleep(5)  # Wait for the page to fully load\n    page_source = driver.page_source\n    soup = BeautifulSoup(page_source, 'html.parser')\n    \n    jobs = []\n    today = datetime.now().date()\n    job_tiles = soup.find_all('div', class_='job-tile')\n    \n    logging.info(f\"Found {len(job_tiles)} job tiles on the page\")\n\n    for job in job_tiles:\n        title_element = job.find('h3', class_='job-title')\n        location_element = job.find('div', class_='location-and-id')\n        link_element = job.find('a', class_='read-more')\n        posted_date_element = job.find('h2', class_=\"posting-date\")\n        description_element = job.find('div', class_=\"description\")\n        if not title_element or not location_element or not link_element or not posted_date_element:\n            logging.warning(f\"Skipping job due to missing elements: {job}\")\n            continue\n\n        title = title_element.text.strip()\n        location = location_element.text.strip().replace('Locations', '').split('|')[0]\n        link = 'https://www.amazon.jobs' + link_element['href']\n        posted_date_str = posted_date_element.text.replace('Posted ', '').strip()\n        description = description_element.text.strip()\n        \n        # Parse the posted date\n        try:\n            posted_date = datetime.strptime(posted_date_str, '%B %d, %Y').date()\n        except ValueError:\n            logging.warning(f\"Skipping job with unrecognized date format: {posted_date_str}\")\n            continue\n\n        # Log dates for debugging\n        logging.info(f\"Today: {today}, Posted Date: {posted_date}, Difference: {(today - posted_date).days} days\")\n\n        # Only consider jobs posted today or yesterday\n        if (today - posted_date).days > 4:\n            logging.info(f\"Skipping job posted on {posted_date_str} (more than 1 day old)\")\n            continue\n        \n        jobs.append({\n            'title': title,\n            'location': location,\n            'link': link,\n            'posted-date': posted_date_str,\n            'overview-text': description\n        })\n    \n    logging.info(f\"Found {len(jobs)} relevant job postings from Amazon\")\n    return {'Amazon': jobs}\n```\n:::\n\n\nFinally, we run the code and print out the jobs that were pulled from the portal.\n\n::: {#77907cd9 .cell execution_count=3}\n``` {.python .cell-code}\ndriver = create_webdriver()\namazon_jobs = get_job_postings_amazon(driver)\npprint.pp(amazon_jobs['Amazon'][0]) #Only printing the first job posting for legibility.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'title': 'Sr. Data Scientist , SCOT - AIM - NA',\n 'location': 'Bellevue, WA, USA',\n 'link': 'https://www.amazon.jobs/en/jobs/2899067/sr-data-scientist-scot-aim-na',\n 'posted-date': 'February 12, 2025',\n 'overview-text': 'Basic qualifications:7+ years of data scientist or similar '\n                  'role involving data extraction, analysis, statistical '\n                  \"modeling and communication experienceMaster's degree in a \"\n                  'quantitative field such as statistics, mathematics, data '\n                  'science, business analytics, economics, finance, '\n                  'engineering, or computer science5+ years of data querying '\n                  'languages (e.g. SQL), scripting languages (e.g. Python) or '\n                  'statistical/mathematical software (e.g. R, SAS, Matlab, '\n                  'etc.) experience...Read more'}\n```\n:::\n:::\n\n\n### Sending the email\nNow that we have some job postings to work with, we can put that into an email template and send the results.\n\n::: {#96435078 .cell execution_count=4}\n``` {.python .cell-code}\nimport smtplib\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nimport logging\nfrom datetime import datetime\nimport pytz\nfrom dotenv import load_dotenv\nimport os\nload_dotenv('_environment.local')\n\ndef send_email(job_postings, sender_email, receiver_email, email_password):\n    tz = pytz.timezone('America/New_York')\n    now = datetime.now(tz)\n    current_time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    # Create the email content\n    message = MIMEMultipart('alternative')\n    message['Subject'] = 'New Job Postings Alert - ' + current_time\n    message['From'] = sender_email\n    message['To'] = receiver_email\n\n    html = \"\"\"\n    <html>\n    <head>\n        <style>\n            body {\n                font-family: Arial, sans-serif;\n                background-color: #f4f4f9;\n                margin: 0;\n                padding: 0;\n            }\n            .container {\n                width: 80%;\n                margin: 0 auto;\n                background-color: #fff;\n                padding: 20px;\n                box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);\n                border-radius: 8px;\n            }\n            h2, h3 {\n                color: #333;\n            }\n            ul {\n                list-style-type: none;\n                padding: 0;\n            }\n            li {\n                background-color: #fafafa;\n                margin: 10px 0;\n                padding: 10px;\n                border-radius: 4px;\n                border: 1px solid #ddd;\n            }\n            a {\n                text-decoration: none;\n                color: #0366d6;\n                font-weight: bold;\n            }\n            a:hover {\n                text-decoration: underline;\n            }\n            .job-title {\n                font-size: 16px;\n                margin: 0;\n            }\n            .job-details {\n                color: #555;\n                font-size: 14px;\n            }\n        </style>\n    </head>\n    <body>\n        <div class=\"container\">\n            <h2>New Job Postings</h2>\n    \"\"\"\n\n    # Debugging: Track the HTML generation\n    logging.info(\"Generating email HTML content\")\n\n    for company, jobs in job_postings.items():\n        html += f\"<h3>{company}</h3><ul>\"\n        for job in jobs:\n            overview_text = job['overview-text']\n            if len(overview_text) > 200:  # Truncate to 200 characters\n                overview_text = overview_text[:200] + '...'\n\n            job_html = f\"\"\"\n            <li>\n                <p class=\"job-title\"><a href='{job['link']}'>{job['title']}</a></p>\n                <p class=\"job-details\">{job['location']}<br>Posted: {job['posted-date']}<br>{overview_text}</p>\n            </li>\n            \"\"\"\n            html += job_html\n            logging.info(f\"Added job HTML: {job_html.strip()}\")  # Debugging: Log each job HTML\n\n        html += \"</ul>\"\n\n    html += \"\"\"\n        </div>\n    </body>\n    </html>\n    \"\"\"\n\n    logging.info(f\"Final email HTML content length: {len(html)}\")  # Debugging: Log final HTML length\n\n    message.attach(MIMEText(html, 'html'))\n\n    # Send the email\n    try:\n        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:\n            server.login(sender_email, email_password)\n            server.sendmail(sender_email, receiver_email, message.as_string())\n        logging.info(\"Email sent successfully\")\n    except Exception as e:\n        logging.error(f\"Failed to send email: {e}\")\n\njob_postings = {}\njob_postings.update(amazon_jobs)\nsend_email(job_postings, os.getenv('SENDER_EMAIL'), os.getenv('RECEIVER_EMAIL'), os.getenv('EMAIL_PASSWORD'))\n```\n:::\n\n\nAnd here is a screenshot of the email that was sent.\n\n![](email_screenshot.png){}\n\n### Deployment\nNow that we have working code to scrape a job portal and a function to format it and send it as an email, we need a plan to deploy the project. To deploy the project, I containerized the app using Docker and then used a combination of GCP Cloud Run to run the container and Cloud Scheduler to run it once a day on a cron schedule.\n\n## Future Directions\nI definitely don't see this as a finished product. In the future, I plan to continue adding new companies; there are just so many cool companies doing exciting, groundbreaking work in machine learning and data science. I also have plans to integrate the Notion API to send the scraped posts directly to the Notion database I use to organize my job applications. I would also like to experiment with integrating the OpenAI API to have GPT-4 score each job posting based on how qualified I am for the position by comparing each job to my resume. So stay tuned for a post on those updates!\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}