{
  "hash": "790b17eba247c56cf41a461a1b095f20",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Quantization of LLMs\nauthor: Noah Olsen\ndate: '2025-02-15'\ncategories:\n  - Generative AI\n  - Machine Learning\n  - DRAFT\n---\n\n## Intro\nI recently had a job interview where I was asked to go into the technical details about the quantization of LLMs. It  had been a while since I had looked at quantization so I blanked a little bit and the details that I did provide were wrong. So I thought this would be a great moment for a blog post on quantization to drive it back into my own brain.\n\n## High Level Background\nThe main goal of quantization is to make large language models (LLMs) more efficient by reducing their memory footprint and computational requirement. As LLMs have continued to improve and model developers pursue the scaling theory that more parameters leads to more performance, many of the state of the art models cannot be run for inference efficiently with the standard computer hardware that the vast majority of people have access to.\n\nFor example, Meta's largest and most advanced model Llama 3.1 405B would require at least 8 NVIDIA A100 GPUs, far beyond what most people dabbling with LLMs have access to.\n\nThe idea behind quantization is to reduce the memory requirements of running inference on a model by lowering the precision of model weights from FP32 to FP8. FP32 allows for high numerical precision, but this comes at the cost of increased memory usage. This tradeoff becomes particularly challenging at scaleâ€”for example, storing 405 billion FP32 weights in a massive LLM requires enormous memory and computational resources.\n\n## What is Floating Point Precision?\nNow that we have a basic understanding of what we are trying to do with quantization, it is important to understand the concepts behind floating point precision and what characteristics make different Floating Point Precision formats different for the sake of LLM inference.\n\n::: {#ac823cea .cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Define bit structures for FP32, FP16, and FP8\nbit_structures = {\n    \"FP32\": {\"Sign\": 1, \"Exponent\": 8, \"Mantissa\": 23},\n    \"FP16\": {\"Sign\": 1, \"Exponent\": 5, \"Mantissa\": 10},\n    \"FP8\": {\"Sign\": 1, \"Exponent\": 5, \"Mantissa\": 2},\n}\n\n# Define a color palette for Sign, Exponent, and Mantissa\ncolors = {\"Sign\": \"#E74C3C\", \"Exponent\": \"#F1C40F\", \"Mantissa\": \"#3498DB\"}\n\n# Create figure and axis\nfig, ax = plt.subplots(figsize=(10, 5))\n\n# Define spacing and row positions\ny_positions = {\"FP32\": 2, \"FP16\": 1, \"FP8\": 0}\nbox_size = 0.9\n\n# Plot individual boxes for each bit\nfor fmt, parts in bit_structures.items():\n    x = 0  # Start position for each format\n    for part, size in parts.items():\n        for i in range(size):\n            rect = patches.Rectangle((x, y_positions[fmt]), box_size, box_size, \n                                     linewidth=1, edgecolor=\"black\", facecolor=colors[part])\n            ax.add_patch(rect)\n            x += 1\n\n# Formatting\nax.set_xlim(-1, 34)\nax.set_ylim(-1, 3)\nax.set_xticks([])\nax.set_yticks([y_positions[\"FP32\"], y_positions[\"FP16\"], y_positions[\"FP8\"]])\nax.set_yticklabels([\"FP32\", \"FP16\", \"FP8\"], fontsize=12)\nax.set_frame_on(False)\n\n# Add legend\nlegend_patches = [patches.Patch(color=colors[key], label=key) for key in colors]\nax.legend(handles=legend_patches, loc=\"lower right\", title=\"Bit Breakdown\")\n\n# Show the plot\nplt.title(\"Bit Breakdown of FP32, FP16, and FP8\")\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=812 height=409}\n:::\n:::\n\n\n::: {#9e8013b8 .cell execution_count=2}\n``` {.python .cell-code}\nprint(\"Hello World\")\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHello World\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}