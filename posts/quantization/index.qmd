---
title: "Quantization of LLMs"
author: "Noah Olsen"
date: "2025-02-15"
categories: [Generative AI, Machine Learning, DRAFT]
jupyter: python3
---

## Intro
I recently had a job interview where I was asked to go into the technical details about the quantization of LLMs. It  had been a while since I had looked at quantization so I blanked a little bit and the details that I did provide were wrong. So I thought this would be a great moment for a blog post on quantization to drive it back into my own brain.

## High Level Background
The main goal of quantization is to make large language models (LLMs) more efficient by reducing their memory footprint and computational requirement. As LLMs have continued to improve and model developers pursue the scaling theory that more parameters leads to more performance, many of the state of the art models cannot be run for inference efficiently with the standard computer hardware that the vast majority of people have access to.

For example, Meta's largest and most advanced model Llama 3.1 405B would require at least 8 NVIDIA A100 GPUs, far beyond what most people dabbling with LLMs have access to.

The idea behind quantization is to reduce the memory requirements of running inference on a model by lowering the precision of model weights from FP32 to FP8. FP32 allows for high numerical precision, but this comes at the cost of increased memory usage. This tradeoff becomes particularly challenging at scaleâ€”for example, storing 405 billion FP32 weights in a massive LLM requires enormous memory and computational resources.

## What is Floating Point Precision?
Now that we have a basic understanding of what we are trying to do with quantization, it is important to understand the concepts behind floating point precision and what characteristics make different Floating Point Precision formats different for the sake of LLM inference.

```{python}

import matplotlib.pyplot as plt
import matplotlib.patches as patches

# Define bit structures for FP32, FP16, and FP8
bit_structures = {
    "FP32": {"Sign": 1, "Exponent": 8, "Mantissa": 23},
    "FP16": {"Sign": 1, "Exponent": 5, "Mantissa": 10},
    "FP8": {"Sign": 1, "Exponent": 5, "Mantissa": 2},
}

# Define a color palette for Sign, Exponent, and Mantissa
colors = {"Sign": "#E74C3C", "Exponent": "#F1C40F", "Mantissa": "#3498DB"}

# Create figure and axis
fig, ax = plt.subplots(figsize=(10, 5))

# Define spacing and row positions
y_positions = {"FP32": 2, "FP16": 1, "FP8": 0}
box_size = 0.9

# Plot individual boxes for each bit
for fmt, parts in bit_structures.items():
    x = 0  # Start position for each format
    for part, size in parts.items():
        for i in range(size):
            rect = patches.Rectangle((x, y_positions[fmt]), box_size, box_size, 
                                     linewidth=1, edgecolor="black", facecolor=colors[part])
            ax.add_patch(rect)
            x += 1

# Formatting
ax.set_xlim(-1, 34)
ax.set_ylim(-1, 3)
ax.set_xticks([])
ax.set_yticks([y_positions["FP32"], y_positions["FP16"], y_positions["FP8"]])
ax.set_yticklabels(["FP32", "FP16", "FP8"], fontsize=12)
ax.set_frame_on(False)

# Add legend
legend_patches = [patches.Patch(color=colors[key], label=key) for key in colors]
ax.legend(handles=legend_patches, loc="lower right", title="Bit Breakdown")

# Show the plot
plt.title("Bit Breakdown of FP32, FP16, and FP8")
plt.show()



```